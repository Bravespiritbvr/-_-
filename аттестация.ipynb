{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузите данные\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"Spark\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|variety|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "|         5.1|        3.5|         1.4|        0.2| Setosa|\n",
      "|         4.9|        3.0|         1.4|        0.2| Setosa|\n",
      "|         4.7|        3.2|         1.3|        0.2| Setosa|\n",
      "|         4.6|        3.1|         1.5|        0.2| Setosa|\n",
      "|         5.0|        3.6|         1.4|        0.2| Setosa|\n",
      "|         5.4|        3.9|         1.7|        0.4| Setosa|\n",
      "|         4.6|        3.4|         1.4|        0.3| Setosa|\n",
      "|         5.0|        3.4|         1.5|        0.2| Setosa|\n",
      "|         4.4|        2.9|         1.4|        0.2| Setosa|\n",
      "|         4.9|        3.1|         1.5|        0.1| Setosa|\n",
      "|         5.4|        3.7|         1.5|        0.2| Setosa|\n",
      "|         4.8|        3.4|         1.6|        0.2| Setosa|\n",
      "|         4.8|        3.0|         1.4|        0.1| Setosa|\n",
      "|         4.3|        3.0|         1.1|        0.1| Setosa|\n",
      "|         5.8|        4.0|         1.2|        0.2| Setosa|\n",
      "|         5.7|        4.4|         1.5|        0.4| Setosa|\n",
      "|         5.4|        3.9|         1.3|        0.4| Setosa|\n",
      "|         5.1|        3.5|         1.4|        0.3| Setosa|\n",
      "|         5.7|        3.8|         1.7|        0.3| Setosa|\n",
      "|         5.1|        3.8|         1.5|        0.3| Setosa|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "data_shema = [StructField(\"sepal_length\",DoubleType(),True), StructField(\"sepal_width\",DoubleType(),True), StructField(\"petal_length\",DoubleType(),True), StructField(\"petal_width\",DoubleType(),True), StructField(\"variety\",StringType(),True)]\n",
    "final_struc = StructType(fields = data_shema)\n",
    "df = spark.read.csv(\"iris.csv\",sep=',',header=True, schema=final_struc)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-------+------------+-----------------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|variety|varietyIndex|         features|\n",
      "+------------+-----------+------------+-----------+-------+------------+-----------------+\n",
      "|         5.1|        3.5|         1.4|        0.2| Setosa|         0.0|[5.1,3.5,1.4,0.2]|\n",
      "|         4.9|        3.0|         1.4|        0.2| Setosa|         0.0|[4.9,3.0,1.4,0.2]|\n",
      "|         4.7|        3.2|         1.3|        0.2| Setosa|         0.0|[4.7,3.2,1.3,0.2]|\n",
      "|         4.6|        3.1|         1.5|        0.2| Setosa|         0.0|[4.6,3.1,1.5,0.2]|\n",
      "|         5.0|        3.6|         1.4|        0.2| Setosa|         0.0|[5.0,3.6,1.4,0.2]|\n",
      "|         5.4|        3.9|         1.7|        0.4| Setosa|         0.0|[5.4,3.9,1.7,0.4]|\n",
      "|         4.6|        3.4|         1.4|        0.3| Setosa|         0.0|[4.6,3.4,1.4,0.3]|\n",
      "|         5.0|        3.4|         1.5|        0.2| Setosa|         0.0|[5.0,3.4,1.5,0.2]|\n",
      "|         4.4|        2.9|         1.4|        0.2| Setosa|         0.0|[4.4,2.9,1.4,0.2]|\n",
      "|         4.9|        3.1|         1.5|        0.1| Setosa|         0.0|[4.9,3.1,1.5,0.1]|\n",
      "|         5.4|        3.7|         1.5|        0.2| Setosa|         0.0|[5.4,3.7,1.5,0.2]|\n",
      "|         4.8|        3.4|         1.6|        0.2| Setosa|         0.0|[4.8,3.4,1.6,0.2]|\n",
      "|         4.8|        3.0|         1.4|        0.1| Setosa|         0.0|[4.8,3.0,1.4,0.1]|\n",
      "|         4.3|        3.0|         1.1|        0.1| Setosa|         0.0|[4.3,3.0,1.1,0.1]|\n",
      "|         5.8|        4.0|         1.2|        0.2| Setosa|         0.0|[5.8,4.0,1.2,0.2]|\n",
      "|         5.7|        4.4|         1.5|        0.4| Setosa|         0.0|[5.7,4.4,1.5,0.4]|\n",
      "|         5.4|        3.9|         1.3|        0.4| Setosa|         0.0|[5.4,3.9,1.3,0.4]|\n",
      "|         5.1|        3.5|         1.4|        0.3| Setosa|         0.0|[5.1,3.5,1.4,0.3]|\n",
      "|         5.7|        3.8|         1.7|        0.3| Setosa|         0.0|[5.7,3.8,1.7,0.3]|\n",
      "|         5.1|        3.8|         1.5|        0.3| Setosa|         0.0|[5.1,3.8,1.5,0.3]|\n",
      "+------------+-----------+------------+-----------+-------+------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#При помощи VectorAssembler преобразовать все колонки с признаками в одну (использовать PipeLine)\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "ignore = ['variety']\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[x for x in df.columns if x not in ignore],\n",
    "    outputCol='features')\n",
    "\n",
    "#assembler.transform(df)\n",
    "\n",
    "#Convert to numeric\n",
    "qualification_indexer = StringIndexer(inputCol=\"variety\", outputCol=\"varietyIndex\")\n",
    "\n",
    "#Create pipeline and pass it to stages\n",
    "pipeline = Pipeline(stages=[\n",
    "           qualification_indexer, \n",
    "           assembler\n",
    "])\n",
    "#fit and transform\n",
    "df_transformed = pipeline.fit(df).transform(df)\n",
    "df_transformed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Разбить данные на train и test\n",
    "train, test = df_transformed.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Создать модель логистической регресии и обучить ее\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(featuresCol='features',\n",
    "                        labelCol='varietyIndex')\n",
    "model = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: 0.9550406020994258\n"
     ]
    }
   ],
   "source": [
    "#Воспользоваться MulticlassClassificationEvaluator для оценки качества на train и test множестве\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "predictions = model.transform(test)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='varietyIndex')\n",
    "print('Evaluation:', evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
